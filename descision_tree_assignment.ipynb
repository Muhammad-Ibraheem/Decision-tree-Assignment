{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de880dc",
   "metadata": {},
   "source": [
    "# Description:\n",
    "employed for both classification and regression applications, a decision tree classifier is a machine learning technique. The model, which resembles a tree, divides the data into subsets according to the values of input features and then goes back and generates predictions. Using a tree structure formed by the algorithm, each leaf node indicates the expected class or value, and each interior node denotes a choice made in response to a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d3ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665633c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413b8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2870eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660ddd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee15b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1fee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe82e24",
   "metadata": {},
   "source": [
    "## Mathematical Intuition\n",
    "comprehending how the algorithm decides to divide the data into distinct classes by optimizing particular criteria is essential to comprehending the mathematical intuition behind decision tree classification. Here is a detailed explanation:\n",
    "\n",
    "# Impurity or Purity Measure:\n",
    "Recursively dividing the data into subsets according to the values of input features is how decision trees are constructed. A measure of purity or impurity is used to decide how to partition the data.\n",
    "Gini impurity and entropy are the two commonly used impurity indices for classification issues.\n",
    "# Gini Impurity:\n",
    "Gini impurity, denoted as Gini(D), measures the probability of misclassifying a randomly chosen element from the dataset. It's calculated as:\n",
    "#### Gini(D) = 1 - Σ(p_i)^2\n",
    "# Entropy:\n",
    "The average amount of information (or disorder) in the dataset is measured by entropy, often known as H(D). It is computed as:\n",
    "#### H(D) = -Σ(p_i * log2(p_i))\n",
    "where the likelihood that a data point belongs to class I is denoted by p_i.\n",
    "# Splitting Criterion:\n",
    "The feature and feature value that minimizes the impurity of the resulting subsets (children nodes) or maximizes the information gain are chosen using the decision tree method.\n",
    "The decrease in impurity from the parent node to the child nodes is known as the information gain, and it can be computed as follows:\n",
    "#### Information Gain = Impurity(parent) - Σ(Weighted Impurity(child))\n",
    "where a child's weighted impurity is calculated by multiplying its impurity by the percentage of data points that it contains.\n",
    "# Splitting Process:\n",
    "The method finds the feature and value that yield the maximum information gain by iteratively going over each feature and threshold.\n",
    "The data is divided into child nodes based on the chosen feature and value.\n",
    "# Recursion:\n",
    "Recursive splitting means that it keeps going for every child node until a certain amount of samples per leaf or maximum depth is reached, for example.\n",
    "# Leaf Nodes:\n",
    "The decision tree's leaf nodes hold the anticipated class labels when the procedure stops splitting.\n",
    "The anticipated class for classification is often determined by looking at the majority class of a leaf node.\n",
    "# Predictions:\n",
    "It moves through the decision tree from the root node to a leaf node in order to forecast a new data point.\n",
    "It follows the relevant branch at each internal node after verifying the value of the related feature in the data point.\n",
    "The procedure is repeated until a leaf node is reached, at which point the class found there is the anticipated class.\n",
    "Optimizing feature and value selection to reduce impurity and arrive at the best classification conclusions for data points is the mathematical intuition behind decision tree classification. Information theory and probability principles serve as the foundation for this procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a26cfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decision tree classifier\n",
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample data (replace this with your dataset)\n",
    "X = [[1, 2], [2, 2], [2, 3], [3, 3], [3, 1]]\n",
    "y = [0, 0, 1, 1, 0]  # Binary class labels (0 or 1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print classification report for more detailed evaluation\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea48011",
   "metadata": {},
   "source": [
    "#  geometric intuition behind decision tree classification \n",
    "Seeing the decision boundaries the tree creates as a sequence of hyperplanes or axis-aligned divisions in the feature space is the geometric idea behind decision tree classification. Gaining insight from this geometric viewpoint might help make decision tree prediction processes more understandable.\n",
    "## Feature Space:\n",
    "See the data in your dataset as points in a feature space that has several dimensions, with each feature denoting a distinct dimension.\n",
    "## Decision Boundaries:\n",
    "By drawing decision borders, a decision tree partitions or divides the feature space into areas.\n",
    "Every decision boundary divides the feature space into two or more regions using a hyperplane.\n",
    "You will have two zones for binary classification, one for each class.\n",
    "## Axis-Aligned Splits:\n",
    "At every internal node, decision tree classifiers base their binary judgments on a single feature. The feature space is partitioned by these choices along the axis.\n",
    "Every internal node has a threshold value and a corresponding characteristic. A data point follows one branch if its feature value is less than or equal to the threshold; if not, it follows the other branch.\n",
    "## Leaf Nodes:\n",
    "Subsequent decision boundaries at additional internal nodes keep dividing the regions formed by these splits.\n",
    "Until the data points reach leaf nodes, the procedure is repeated recursively.\n",
    "Every leaf node has a corresponding class label. A leaf node's majority class is frequently selected as the projected class.\n",
    "## Prediction:\n",
    "You follow the decision tree's path from the root node to a leaf node in order to anticipate a new data point.\n",
    "You choose the appropriate branch at each internal node by comparing the value of the related feature in the data point to the threshold.\n",
    "Until the data point reaches a leaf node, this process keeps going. The last prediction for the data point is the class label linked to that leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926cb631",
   "metadata": {},
   "source": [
    "# confusion matrix\n",
    "A confusion matrix is a method for evaluating a classification model's performance that is used in binary and multi-class classification. It offers a tabular display of the model's predictions in relation to the dataset's actual class labels. Understanding the true positives, true negatives, false positives, and false negatives that a classifier produces is made easier with the help of the matrix.\n",
    "Typically, a confusion matrix is set up like this:\n",
    "##              Predicted Class\n",
    "            |  Positive  |  Negative  |\n",
    "Actual    ---------------------------------\n",
    "Class     |            |            |\n",
    "-----------------------------------------\n",
    "Positive  | True       | False      |\n",
    "Class     | Positives  | Negatives  |\n",
    "-----------------------------------------\n",
    "Negative  | False      | True       |\n",
    "Class     | Positives  | Negatives  |\n",
    "-----------------------------------------\n",
    "The words that are used in a confusion matrix are defined as follows:\n",
    "-The cases that the model accurately predicted to belong to the positive class are known as True Positives (TP)\n",
    "-The cases that the model accurately predicted to be in the negative class are known as True Negatives (TN).\n",
    "-False Positives (FP): These are the cases where the model predicted erroneously that a given instance belonged in the positive class when in fact it did not. also referred to as Type 1 mistakes.\n",
    "-False Negatives (FN): These are the cases where the model predicted a positive class when it should have been a negative class. Named after Type II mistakes as well.\n",
    "### The confusion matrix allows you to assess various aspects of a classification model's performance:\n",
    "-Accuracy: Measured as (TP + TN) / (TP + TN + FP + FN), accuracy indicates how well the model detects cases. It offers a broad assessment of the model's accuracy, although it might not be the most useful metric for datasets that are unbalanced.\n",
    "-Precision is the model's ability to accurately identify positive examples among all the instances it expects to be positive. The formula for it is TP / (TP + FP). Reduced false positive error rates are indicative of high precision.\n",
    "-Recall, also known as True Positive Rate or Sensitivity, quantifies the model's capacity to accurately detect every positive instance among all real positive instances. The formula for it is TP / (TP + FN). Low false negative error rates are correlated with high recall.\n",
    "-Specificity (True Negative Rate): This quantifies how well the model can detect every negative instance among all real negative instances. The formula is TN / (TN + FP).\n",
    "-F1 Score: The F1 score strikes a balance between recall and precision by taking the harmonic mean of the two. 2 * (precision * recall) / (precision + recall) is the calculation.\n",
    "-Sensitivity and Specificity: These two metrics are frequently employed in diagnostic and medical procedures. As previously said, specificity and sensitivity are equivalent to recall and specificity, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fcd4c1",
   "metadata": {},
   "source": [
    "# example of a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af428da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3 1]\n",
      " [2 4]]\n",
      "Precision: 0.8\n",
      "Recall: 0.6666666666666666\n",
      "F1 Score: 0.7272727272727272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Actual and predicted labels\n",
    "y_true = [1, 0, 1, 0, 1, 0, 0, 1, 1, 1]\n",
    "y_pred = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "\n",
    "# Calculate recall (sensitivity)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9881e",
   "metadata": {},
   "source": [
    "# choosing an appropriate evaluation metric\n",
    "Selecting the right evaluation metric for a classification problem is essential since it establishes how a model's performance is evaluated and if it is adequate for a certain application. Since different metrics concentrate on different aspects of classification performance, the choice of metric should be in line with the objectives and features of the problem. Here are some reasons why choosing the appropriate assessment metric matters and some tips for doing so:\n",
    "## Alignment with Problem Goals:\n",
    "The priorities of various classification tasks may differ. In the context of medical diagnosis, the cost of misdiagnosing a healthy patient (false positive) may be significantly lower than that of missing an illness (false negative). Metrics like recall or sensitivity would be given priority in these situations.\n",
    "To prevent vital emails from being overlooked, you may be more concerned with stopping false positives in spam email detection—legitimate emails that are mistakenly labeled as spam. You could therefore give accuracy priority.\n",
    "## Imbalanced Datasets:\n",
    "Accuracy may not be the best statistic in imbalanced datasets, where one class has far less samples than the other. Because the majority class predominates in certain situations, accuracy may be high even while the model incorrectly categorizes the minority class. In imbalanced datasets, metrics such as precision, recall, F1 score, or AUC-ROC (Area Under the Receiver Operating Characteristic curve) are frequently more revealing.\n",
    "## Threshold Selection:\n",
    "Rather than producing binary predictions, many classification techniques generate probability scores. The model's performance can be affected by the probability threshold that is selected for classifying cases. Certain measures, such as the ROC curve, assist you in evaluating model performance over a range of threshold values.\n",
    "## Validation and Cross-Validation:\n",
    "Validation or cross-validation is a crucial step in model evaluation that guarantees the selected metric is consistently dependable across various data subsets. This lessens the chance of overfitting to a particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0174ed",
   "metadata": {},
   "source": [
    "# example of a classification problem where precision is the most crucial criterion.\n",
    "## Problem: Spam Email Detection\n",
    "## Explanation:\n",
    "The main goal of spam email detection is to reduce the amount of false positives—that is, emails that are actually legitimate but are mistakenly identified as spam. This is due to the potentially dire effects of false positives:\n",
    "Loss of Vital Information: If a valid email is wrongly interpreted as spam, the recipient may be deprived of vital information, including updates, personal correspondence, and work-related correspondence.\n",
    "User Frustration: People may spend time rummaging through spam folders in an attempt to find crucial emails, which can result in false positives. The user experience may suffer as a result.\n",
    "## Business Impact:\n",
    "When it comes to businesses, false positives might result in lost opportunities, unsuccessful transactions, or subpar customer service if orders or questions from clients are incorrectly classified as spam.Precision is an important parameter for spam email identification because of these implications. The percentage of emails accurately identified as spam out of all emails anticipated to be spam is called precision. High accuracy reduces false positives and maintains the integrity of the inbox by guaranteeing that when the model classifies an email as spam, it is very likely to be spam.In this case, precision is more important than other metrics like recall (sensitivity). High recall would guarantee that the majority of spam emails are detected, but it could also lead to a significant amount of false positives, which is inappropriate in this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ff9fe",
   "metadata": {},
   "source": [
    "# example of a classification problem where recall is the most important metric\n",
    "Medical diagnosis, especially for life-threatening disorders, is one setting where memory is the most crucial measure in a classification problem.\n",
    "## Problem: Medical Diagnosis for a Life-Threatening Disease\n",
    "## Explanation:\n",
    "Making sure that all positive instances are accurately detected is the main priority in medical diagnosis, particularly when dealing with life-threatening conditions like cancer or infectious infections. Positive cases in this context refer to those who have the illness and need medical care. False negatives, or missing positive cases, can have detrimental effects.\n",
    "Treatment Delay: Delays in diagnosis and treatment due to false negative results can drastically lower the likelihood of a patient's survival or recovery.\n",
    "Patient Well-Being: Patients' life and well-being are occasionally in jeopardy. If a life-threatening illness is missed, the damage may be irreversible.\n",
    "Public Health Concerns: False negative results for infectious diseases increase the risk to the public's health by aiding in the disease's community spread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefdffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768e31c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc431009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
